#!/usr/bin/env python

from __future__ import print_function
import sisfft
from sisfft._utils import log_sum
import numpy
np = numpy
import argparse
import logging
import hashlib
import multiprocessing
from os import path
from datetime import datetime

np.random.seed(1)
rand = np.random.random

DEFAULT_BETA = 0.001
DEFAULT_LENGTH = 100
DEFAULT_REPEAT = 100
DEFAULT_PROCESSES = min(multiprocessing.cpu_count() / 2, 8)

def constant(length):
    return np.ones(length)

def uniform(length):
    return -40 * rand(length)

def quadratic(length):
    x = np.linspace(0, 1.0, length)
    x2 = x * x
    return -(30 * rand() + 30) * x2 + 40 * (rand() - 0.5) * x + 40 * (rand() - 0.5)

def sinusoid(length):
    x = np.linspace(0, 3 * np.pi, length)
    return 10 * (1 + 3*rand()) * np.sin(x + rand() * 0.1) - 10 * (-1 + 5*rand()) * x

def multi_scales(length):
    v = np.append(-30 * rand(length / 5), -100 - 100 * rand(length - length / 5))
    np.random.shuffle(v)
    return v

def multier_scales(length):
    v = np.append(-15 * (1 + 2*rand()) * rand(length / 3),
                  -50 * (1 + 2 * rand()) -50 * (1 + 2 * rand()) * rand(length - length / 3))
    np.random.shuffle(v)
    return v

def shaped_multi_scales(length):
    return multier_scales(length) + quadratic(length)

def write_header(f):
    print('beta,psFFTC_rel_max,psFFTC_pvalue_max,FFTC_rel_max,FFTC_pvalue_max,failed', file = f)
    f.flush()

print_count = 0
def write_results(f, beta, psFFTC_rel_max, psFFTC_pvalue_max, FFTC_rel_max, FFTC_pvalue_max):
    global print_count
    failed = psFFTC_rel_max > np.log(beta)
    s = '{beta},{psFFTC_rel_max},{psFFTC_pvalue_max},{FFTC_rel_max},{FFTC_pvalue_max},{failed}'.format(
        beta = beta,
        psFFTC_rel_max=psFFTC_rel_max,psFFTC_pvalue_max=psFFTC_pvalue_max,
        FFTC_rel_max=FFTC_rel_max,FFTC_pvalue_max=FFTC_pvalue_max,
        failed = 1 if failed else 0
    )
    print(s, file=f)
    print_count += 1
    if print_count % 100 == 0:
        f.flush()
    print(s)
    return failed

def rel(log_attempt, log_true):
    return np.log(np.abs(np.expm1(log_attempt - log_true)))

def all_pvalues(v):
    return np.array([log_sum(v[i:]) for i in range(len(v))])

def run(pmf1_name, pmf1s, pmf2_name, pmf2s, date, n, repeat, betas, out_dir):
    np.random.seed(1)
    seen1 = set()

    fname = 'psfft-accuracy-{date}-length-{n}-repeat-{repeat}-pmf1-{pmf1}-pmf2-{pmf2}'.format(
        date = date,
        n = n,
        repeat = repeat,
        pmf1 = pmf1_name,
        pmf2 = pmf2_name)
    failures = 0
    with open(path.join(out_dir, fname + '.csv'), 'w') as f:
        write_header(f)
        for pmf1 in pmf1s:
            h = hashlib.sha256()
            h.update(pmf1.data)
            digest = h.hexdigest()
            if digest in seen1:
                continue
            else:
                seen1.add(digest)

            seen2 = set()
            for pmf2 in pmf2s:
                h = hashlib.sha256()
                h.update(pmf2.data)
                digest = h.hexdigest()
                if digest in seen2:
                    continue
                else:
                    seen2.add(digest)

                nc = sisfft._naive.convolve_naive(pmf1, pmf2)
                nc_pvalues = all_pvalues(nc)

                fftc = sisfft._naive.convolve_fft(pmf1, pmf2)
                fftc_pvalues = all_pvalues(fftc)
                fftc_rel_max = rel(fftc, nc).max()
                fftc_pvalue_max = rel(fftc_pvalues, nc_pvalues).max()

                for beta in betas:
                    psfftc = sisfft.log_convolve(pmf1, pmf2, 1.0 / beta)
                    psfftc_pvalues = all_pvalues(psfftc)
                    psfftc_rel_max = rel(psfftc, nc).max()
                    psfftc_pvalue_max = rel(psfftc_pvalues, nc_pvalues).max()

                    failed = write_results(f,
                                           beta,
                                           psfftc_rel_max, psfftc_pvalue_max,
                                           fftc_rel_max, fftc_pvalue_max)
                    if failed:
                        name = fname + '-beta-{beta}-count-{failures}-'.format(
                            beta = beta,
                            failures = failures)
                        failures += 1
                        np.save(path.join(out_dir, name + '1'), pmf1)
                        np.save(path.join(out_dir, name + '1'), pmf2)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-r', '--repeat', type=int, default=DEFAULT_REPEAT,
                        help = 'the number of random vectors')
    parser.add_argument('-n', '--length', type=int, action='append',
                        help = 'length of the vectors')
    parser.add_argument('-o', '--out-dir', metavar='DIR', default='.',
                        help = 'directory to write files to')
    parser.add_argument('-b', '--beta', metavar='ACCURACY', type=float, action='append',
                        help = 'relative accuracies to compute the p-value to')
    parser.add_argument('-p', '--processes', metavar='COUNT', type=int, default=DEFAULT_PROCESSES,
                        help = 'number of processes to run tests on')
    parser.add_argument('--log', default='WARN',
                        help = 'level to log at')
    parser.add_argument('pmfs', metavar='PMF', nargs='+',
                        help = 'python expressions for function that create vectors/PMFs to benchmark')

    args = parser.parse_args()

    level = getattr(logging, args.log.upper(), None)
    if not isinstance(level, int):
        raise ValueError('--log argument was not a valid level')
    logging.basicConfig(level = level)

    repeat = args.repeat
    lengths = args.length or [DEFAULT_LENGTH]
    betas = args.beta or [DEFAULT_BETA]
    out_dir = args.out_dir

    pmfs = [(name, eval(name)) for name in args.pmfs]
    date = datetime.now().strftime('%Y%m%d-%H%M%S')

    P = multiprocessing.Pool(args.processes)

    def normalise(v):
        return v - log_sum(v)
    jobs = []
    for n in lengths:
        np.random.seed(1)
        numeric_pmfs = [(name, [normalise(func(n)) for _ in range(repeat)]) for name, func in pmfs]

        for i, (pmf1_name, pmf1s) in enumerate(numeric_pmfs):
            for pmf2_name, pmf2s in numeric_pmfs[i:]:
                print(n, pmf1_name, pmf2_name)
                run_args = (pmf1_name, pmf1s, pmf2_name, pmf2s, date, n, repeat, betas, out_dir)
                if args.processes > 1:
                    jobs.append(P.apply_async(run, run_args))
                else:
                    run(*run_args)


    for j in jobs:
        j.get()
    P.close()
    P.join()

if __name__ == '__main__':
    main()
